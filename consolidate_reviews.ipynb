{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197974b",
   "metadata": {},
   "source": [
    "## Show all review CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df = pd.read_csv(\"csv_files/google.csv\", encoding='latin-1') #need latin-1 encoding for the google reviews found through trial & error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv(\"csv_files/yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb570c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74957a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = pd.read_csv(\"csv_files/trip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ce1d1",
   "metadata": {},
   "source": [
    "### We want to consolidate all of the reviews into one big CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36627935",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_df), len(yelp_df), len(trip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4dcba",
   "metadata": {},
   "source": [
    "Convert 'date' column to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd856978",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df['date'] = pd.to_datetime(google_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df['date'] = pd.to_datetime(yelp_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df['date'] = pd.to_datetime(trip_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = pd.concat([google_df, yelp_df, trip_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55575ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bc4b5",
   "metadata": {},
   "source": [
    "642 rows matches the summed lengths of each individual dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32582289",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df[490: 510] # Where google reviews meet yelp reviews in the new consolidated df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df['review_text'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada6e6d",
   "metadata": {},
   "source": [
    "Let's get rid of the rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = consolidated_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df) # Should be 642 - 217 = 425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d05310",
   "metadata": {},
   "source": [
    "Now our data is all consolidated and the rows with missing review text have been removed. All that's left is to sort it by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = cleaned_df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5537137",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save sorted df to csv\n",
    "# sorted_df.to_csv(\"sorted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment scores based on 'rating' column\n",
    "df = pd.read_csv(\"sorted.csv\")\n",
    "df['sentiment'] = df['rating'].apply(lambda x: 'positive' if x >= 4 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f94266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a copy of the dataframe in case we need to revert back\n",
    "df_copy = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3f3cc",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "Let's clean the text to prepare it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, punctuation, and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef659e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_review_text'] = df['review_text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc2692",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "Let's explore the data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3805b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_distribution = df['sentiment'].value_counts(normalize=True) # `normalize=True` returns percentages, `False` returns raw counts\n",
    "sentiment_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2572342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ed376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to 'datetime' dtype if needed\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Compare sentiment distribution across different time periods (e.g., months or years)\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "sentiment_by_month = df.groupby(['year_month', 'sentiment']).size().unstack(fill_value=0)\n",
    "sentiment_by_month_percentage = sentiment_by_month.div(sentiment_by_month.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot sentiment distribution over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_by_month_percentage.plot(kind='bar', stacked=True)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ccf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_ngrams(text, sentiment, pos_or_neg, ngram_range=(1, 2)):\n",
    "    # Filter reviews by sentiment\n",
    "    text_filtered = text[sentiment == pos_or_neg]\n",
    "    # Initialize CountVectorizer to generate n-grams\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    # Fit and transform text data to extract n-grams\n",
    "    X = vectorizer.fit_transform(text_filtered)\n",
    "    # Get the feature names\n",
    "    ngrams = vectorizer.get_feature_names_out()    \n",
    "    # Get the count of each n-gram\n",
    "    ngram_counts = X.sum(axis=0).A1\n",
    "    # Create a dict mapping ngram to its count\n",
    "    ngram_freq = dict(zip(ngrams, ngram_counts))\n",
    "    # Sort the dict by frequency in descending order\n",
    "    sorted_ngram_freq = sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427eb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams from cleaned review text\n",
    "ngrams = extract_ngrams(df['cleaned_review_text'], df['sentiment'], 'positive', ngram_range=(2, 2))\n",
    "\n",
    "# Display top 10 most frequent bigrams\n",
    "top_ngrams = ngrams[:10]\n",
    "for ngram, freq in top_ngrams:\n",
    "    print(f'{ngram}: {freq}')\n",
    "\n",
    "# Plot the most frequent bigrams\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(*zip(*top_ngrams), color=\"coral\")\n",
    "plt.title('Top 10 Most Frequent Bigrams in Positive Reviews')\n",
    "plt.xlabel('Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ed1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams from cleaned review text\n",
    "ngrams = extract_ngrams(df['cleaned_review_text'], df['sentiment'], 'negative', ngram_range=(2, 2))\n",
    "\n",
    "# Display top 10 most frequent bigrams\n",
    "top_ngrams = ngrams[:10]\n",
    "for ngram, freq in top_ngrams:\n",
    "    print(f'{ngram}: {freq}')\n",
    "\n",
    "# Plot the most frequent bigrams\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(*zip(*top_ngrams))\n",
    "plt.title('Top 10 Most Frequent Bigrams in Negative (<=3 stars) Reviews')\n",
    "plt.xlabel('Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8ef32",
   "metadata": {},
   "source": [
    "#### Let's try out some Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF) to see if we can find any latent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "\n",
    "from gensim import corpora, models\n",
    "# Filter for negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative']\n",
    "\n",
    "# Tokenize the cleaned review text\n",
    "tokenized_reviews = negative_reviews['cleaned_review_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Create a dictionary mapping words to unique ids\n",
    "dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_reviews]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5 # Specify the number of topics\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8504a1",
   "metadata": {},
   "source": [
    "Now let's try to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wordcloud\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05691e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!C:\\Users\\rwynn\\anaconda3\\python.exe -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Iterate through the topics and create word clouds\n",
    "for topic in lda_model.show_topics(num_topics=num_topics, formatted=False):\n",
    "    topic_words = dict(topic[1])\n",
    "    wordcloud = WordCloud(background_color='white').generate_from_frequencies(topic_words)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Topic ' + str(topic[0]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned review text\n",
    "tfidf_matrix = vectorizer.fit_transform(negative_reviews['cleaned_review_text'])\n",
    "\n",
    "# Specify number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Initialize and fit the NMF model\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "# Print the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_index, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"Topic {topic_index}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a75359",
   "metadata": {},
   "source": [
    "Let's visualize the NMF output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the top words for each topic\n",
    "top_words = []\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words.append([feature_names[i] for i in topic.argsort()[:-11:-1]])\n",
    "\n",
    "# Plot the top words for each topic\n",
    "fig, axs = plt.subplots(nrows=num_topics, figsize=(10, 8))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.barh(range(10), top_words[i][::-1], color='skyblue')\n",
    "    ax.set_title(f'Topic {i}')\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(range(10))\n",
    "    ax.set_yticklabels(top_words[i][::-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096f766",
   "metadata": {},
   "source": [
    "#### Let's try some keyword extraction and see what we can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define keywords for aspect extraction\n",
    "aspect_keywords = {'food': ['food', 'meal', 'dish', 'pizza', 'pizzas', 'wings', 'bread', 'catch a fire', 'catch-a-fire', 'caf', 'appetizer', 'appetizers', 'pie'],\n",
    "                   'service': ['service', 'waiter', 'waitress', 'staff', 'bar staff', 'bartender', 'bartenders'],\n",
    "                   'parking': ['parking', 'parking lot', 'park'],\n",
    "                   'beer': ['beer', 'beers', 'ipa', 'ipas', 'lager', 'stout', 'stouts', 'wine', 'brew', 'brews', 'drink', 'drinks'],\n",
    "                   'cocktails': ['cocktail', 'cocktails', 'sway', 'mocktail', 'mocktails', 'mixed drinks'],\n",
    "                   'dogs': ['dog', 'dogs', 'puppy'],\n",
    "                   'kids': ['kid', 'kids', 'child', 'children'],\n",
    "                   'price': ['price', 'prices', 'value', 'cost'],\n",
    "                   'atmosphere': ['atmosphere', 'place', 'vibe', 'space', 'venue', 'crowd', 'ambiance', 'spot', 'brewery'],}\n",
    "\n",
    "# Function to extract aspects from review text\n",
    "def extract_aspects(review_text):\n",
    "    aspects = []\n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b{}\\b'.format(keyword), review_text, flags=re.IGNORECASE):\n",
    "                aspects.append(aspect)\n",
    "                break\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f10b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply aspect extraction function to review text column\n",
    "df['aspects'] = df['review_text'].apply(extract_aspects)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eec14f",
   "metadata": {},
   "source": [
    "Now we have extracted the various aspects of each review. Let's try to analyze the sentiments associated with each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with review text and extracted aspects\n",
    "test_df = pd.DataFrame({'review_text': [\"The food was excellent but the service was slow.\",\n",
    "                                   \"Great atmosphere but parking was a nightmare.\"],\n",
    "                   'aspects': [['food', 'service'], ['atmosphere', 'parking']]})\n",
    "\n",
    "\n",
    "# Function to perform sentiment analysis at aspect level\n",
    "def analyze_sentiment_aspects(review_text, aspects):\n",
    "    # Placeholder sentiment lexicons (replace with your actual lexicons)\n",
    "    aspect_sentiment_lexicons = {\n",
    "        'food': {'positive': ['excellent', 'tasty'], 'negative': ['slow', 'bland']},\n",
    "        'service': {'positive': ['excellent', 'friendly'], 'negative': ['slow', 'poor']},\n",
    "        'atmosphere': {'positive': ['great', 'pleasant'], 'negative': ['noisy', 'crowded']},\n",
    "        'parking': {'positive': ['convenient', 'ample'], 'negative': ['nightmare', 'limited']}\n",
    "    }\n",
    "\n",
    "    aspect_sentiments = {}\n",
    "    for aspect in aspects:\n",
    "        sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "        for word in review_text.split():\n",
    "            if word.lower() in aspect_sentiment_lexicons.get(aspect, {}).get('positive', []):\n",
    "                sentiment_scores['positive'] += 1\n",
    "            elif word.lower() in aspect_sentiment_lexicons.get(aspect, {}).get('negative', []):\n",
    "                sentiment_scores['negative'] += 1\n",
    "            else:\n",
    "                sentiment_scores['neutral'] += 1\n",
    "        aspect_sentiments[aspect] = sentiment_scores\n",
    "\n",
    "    return aspect_sentiments\n",
    "\n",
    "# Apply sentiment analysis function to each row in the DataFrame\n",
    "test_df['aspect_sentiments'] = test_df.apply(lambda row: analyze_sentiment_aspects(row['review_text'], row['aspects']), axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27075f40",
   "metadata": {},
   "source": [
    "Think we're going to need something a little more in-depth that this simple rules-based analysis. Let's try a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c96ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained AUTO tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Load pre-trained AUTO model for sequence classification (AutoModel will look at the bert-base-uncased modelâ€™s configuration and choose the appropriate base model architecture to use)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment', num_labels=5, ignore_mismatched_sizes=True)  # 3 labels: positive, negative, neutral\n",
    "\n",
    "# Encode some review text into a tensor\n",
    "tokens = tokenizer.encode(\"this place is the worst. terrible\", return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "\n",
    "# Decode the tensor back into review text\n",
    "decoded = tokenizer.decode(tokens[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass tokens to model to return sentiment\n",
    "result = model(tokens)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1690724",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(result.logits) + 1  # 1 is the worst, 5 is the best (as in star reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df2554",
   "metadata": {},
   "source": [
    "Let's continue with the sentiment analysis. We can see that when the statement is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a04c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"i love this product so much\", return_tensors=\"pt\")\n",
    "result = model(tokens)\n",
    "torch.argmax(result.logits) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1ca40",
   "metadata": {},
   "source": [
    "The result is a '5', as in '5 stars'. But when the statement is negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"the food is the worst\", return_tensors=\"pt\")\n",
    "result = model(tokens)\n",
    "torch.argmax(result.logits) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e5e5f",
   "metadata": {},
   "source": [
    "It results in just 1 star. And with a neutral statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2dc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"this place is just ok\", return_tensors=\"pt\")\n",
    "result = model(tokens)\n",
    "torch.argmax(result.logits) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442ff5e",
   "metadata": {},
   "source": [
    "It lands right in the middle with 3 stars. So how can we go about applying this to individual aspect statements? Meaning, not an entire review, but specifically to individual phrases that reference specific aspects of the business. For example, if the review text were: \"The food was good but the service was awful\", we would want to first extract the aspects 'food' and 'service', and then also apply a sentiment to each aspect. In this case, we'd want 'food' to get a 'positive' sentiment and 'service' to get a negative sentiment. How can we achieve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf42b0",
   "metadata": {},
   "source": [
    "## Aspect Term Extract and Sentiment Analysis via [PyABSA](https://github.com/yangheng95/PyABSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyabsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de308676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa import available_checkpoints\n",
    "from pyabsa import TaskCodeOption\n",
    "# Check available checkpoints for a given task code (https://github.com/yangheng95/PyABSA/blob/v2/pyabsa/framework/checkpoint_class/checkpoint_utils.py)\n",
    "# for current version\n",
    "checkpoint_map = available_checkpoints(task_code=TaskCodeOption.Aspect_Term_Extraction_and_Classification, show_ckpts=True)\n",
    "checkpoint_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5343b17",
   "metadata": {},
   "source": [
    "### Extract aspect terms and classify sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7132b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-22 21:03:21] (2.4.1.post1) ********** \u001b[32mAvailable ATEPC model checkpoints for Version:2.4.1.post1 (this version)\u001b[0m **********\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) ********** \u001b[32mAvailable ATEPC model checkpoints for Version:2.4.1.post1 (this version)\u001b[0m **********\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) \u001b[32mDownloading checkpoint:english \u001b[0m\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) \u001b[31mNotice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\u001b[0m\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) config: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.config\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) state_dict: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.state_dict\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) model: None\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.tokenizer\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) Set Model Device: cpu\n",
      "[2024-03-22 21:03:21] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyabsa.tasks.AspectTermExtraction.prediction.aspect_extractor.AspectExtractor at 0x253c902abc0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyabsa import ATEPCCheckpointManager\n",
    "\n",
    "# Init aspect extractor from online checkpoint or local checkpoint\n",
    "aspect_extract = ATEPCCheckpointManager.get_aspect_extractor(checkpoint='english',\n",
    "                                                             auto_device=False  # False means load model on CPU\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba77fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-22 21:08:32] (2.4.1.post1) Can not load en_core_web_sm from spacy, try to download it in order to parse syntax tree: \u001b[32m\n",
      "python -m spacy download en_core_web_sm\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Download failed, you can download en_core_web_sm manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\dataset_utils\\__lcf__\\apc_utils.py:385\u001b[0m, in \u001b[0;36mconfigure_spacy_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspacy_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mname (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\dataset_utils\\__lcf__\\apc_utils.py:397\u001b[0m, in \u001b[0;36mconfigure_spacy_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    396\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython -m spacy download \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config\u001b[38;5;241m.\u001b[39mspacy_model))\n\u001b[1;32m--> 397\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspacy_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mname (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe food is good but the service is terrible\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m inference_source \u001b[38;5;241m=\u001b[39m examples\n\u001b[1;32m----> 4\u001b[0m atepc_result \u001b[38;5;241m=\u001b[39m \u001b[43maspect_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_aspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mpred_sentiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Predict the sentiment of the extracted aspect terms\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:252\u001b[0m, in \u001b[0;36mAspectExtractor.extract_aspect\u001b[1;34m(self, inference_source, save_result, print_result, pred_sentiment, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_aspect\u001b[39m(\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    232\u001b[0m     inference_source: Union[List[Path], \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    237\u001b[0m ):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Extract aspects and their corresponding polarities from a list of input files.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m        The predicted aspects and their corresponding polarities.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_predict(\n\u001b[0;32m    253\u001b[0m         inference_source, save_result, print_result, pred_sentiment, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    254\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:316\u001b[0m, in \u001b[0;36mAspectExtractor.batch_predict\u001b[1;34m(self, target_file, save_result, print_result, pred_sentiment, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run inference using examples list or inference dataset path (list)!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     )\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_file:\n\u001b[1;32m--> 316\u001b[0m     extraction_res, sentence_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pred_sentiment:\n\u001b[0;32m    318\u001b[0m         filtered_res \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:404\u001b[0m, in \u001b[0;36mAspectExtractor._extract\u001b[1;34m(self, examples)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    403\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mget_examples_for_aspect_extraction(examples)\n\u001b[1;32m--> 404\u001b[0m infer_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_ate_examples_to_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m all_spc_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    412\u001b[0m     [f\u001b[38;5;241m.\u001b[39minput_ids_spc \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m infer_features], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m    413\u001b[0m )\n\u001b[0;32m    414\u001b[0m all_segment_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    415\u001b[0m     [f\u001b[38;5;241m.\u001b[39msegment_ids \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m infer_features], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m    416\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\dataset_utils\\__lcf__\\data_utils_for_inference.py:168\u001b[0m, in \u001b[0;36mconvert_ate_examples_to_features\u001b[1;34m(examples, label_list, max_seq_len, tokenizer, config)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_ate_examples_to_features\u001b[39m(\n\u001b[0;32m    164\u001b[0m     examples, label_list, max_seq_len, tokenizer, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    165\u001b[0m ):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;124;03m\"\"\"Loads a raw_data file into a list of `InputBatch`s.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mconfigure_spacy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     bos_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbos_token\n\u001b[0;32m    170\u001b[0m     eos_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\dataset_utils\\__lcf__\\apc_utils.py:399\u001b[0m, in \u001b[0;36mconfigure_spacy_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    397\u001b[0m         nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(config\u001b[38;5;241m.\u001b[39mspacy_model)\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload failed, you can download \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    401\u001b[0m                 config\u001b[38;5;241m.\u001b[39mspacy_model\n\u001b[0;32m    402\u001b[0m             )\n\u001b[0;32m    403\u001b[0m         )\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Download failed, you can download en_core_web_sm manually."
     ]
    }
   ],
   "source": [
    "# Aspect term extract & sentiment inference\n",
    "examples = ['the food is good but the service is terrible']\n",
    "inference_source = examples\n",
    "atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source,\n",
    "                                               pred_sentiment=True # Predict the sentiment of the extracted aspect terms\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c67ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
