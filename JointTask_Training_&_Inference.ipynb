{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kevinscaria/InstructABSA/blob/main/JointTask_Training_%26_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{Scaria2023InstructABSAIL,\n",
    "  title={InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis},\n",
    "  author={Kevin Scaria and Himanshu Gupta and Saurabh Arjun Sawant and Swaroop Mishra and Chitta Baral},\n",
    "  year={2023}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2xc1EQjq3LY"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4oIyN4lu6qm",
    "outputId": "cf2db64e-cc0f-4682-c372-3cae89f597ee"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pTZzGWQTQAxt"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install transformers\n",
    "  !pip install datasets\n",
    "  !pip install evaluate\n",
    "  !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6oTPPfF2q5S5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rwynn\\AppData\\Local\\Temp\\ipykernel_12008\\2756787886.py:9: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  use_mps = True if torch.has_mps else False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "if IN_COLAB:\n",
    "    root_path = 'Enter drive path'\n",
    "else:\n",
    "    root_path = r'C:\\Users\\rwynn\\PycharmProjects\\InstructABSA'\n",
    "    \n",
    "use_mps = True if torch.has_mps else False\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-DvLkxG9lp2t"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "from InstructABSA.data_prep import DatasetLoader\n",
    "from InstructABSA.utils import T5Generator, T5Classifier\n",
    "from instructions import InstructionsHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht-x1qxn_AxG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "knn9pcJMXovr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name:  robs_experiment\n",
      "Model output path:  .\\Models\\joint_task\\kevinscariajoint_tk-instruct-base-def-pos-neg-neut-combined-robs_experiment\n"
     ]
    }
   ],
   "source": [
    "task_name = 'joint_task'\n",
    "experiment_name = 'robs_experiment'\n",
    "model_checkpoint = 'kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined' # originally set to 'allenai/tk-instruct-base-def-pos'\n",
    "print('Experiment Name: ', experiment_name)\n",
    "model_out_path = '.\\Models'\n",
    "model_out_path = os.path.join(model_out_path, task_name, f\"{model_checkpoint.replace('/', '')}-{experiment_name}\")\n",
    "print('Model output path: ', model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3hfAuLAvVe8D"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/Train/mt_train_data.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/Test/mt_test_data.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set2()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_aspe_format(loader.train_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_aspe_format(loader.test_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>aspectTerms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't live in the area and was visiting from...</td>\n",
       "      <td>[{'term': 'outside eating areas', 'polarity': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this place! Come every few weeks to cozy ...</td>\n",
       "      <td>[{'term': 'fire', 'polarity': 'positive'}, {'t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Friendly staff</td>\n",
       "      <td>[{'term': 'staff', 'polarity': 'positive'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't think the aftertaste of Mad Tree's bee...</td>\n",
       "      <td>[{'term': 'beers', 'polarity': 'negative'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Had the pizza, not very happy with quality to ...</td>\n",
       "      <td>[{'term': 'pizza', 'polarity': 'negative'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  I don't live in the area and was visiting from...   \n",
       "1  Love this place! Come every few weeks to cozy ...   \n",
       "2                                     Friendly staff   \n",
       "3  I don't think the aftertaste of Mad Tree's bee...   \n",
       "4  Had the pizza, not very happy with quality to ...   \n",
       "\n",
       "                                         aspectTerms  \n",
       "0  [{'term': 'outside eating areas', 'polarity': ...  \n",
       "1  [{'term': 'fire', 'polarity': 'positive'}, {'t...  \n",
       "2        [{'term': 'staff', 'polarity': 'positive'}]  \n",
       "3        [{'term': 'beers', 'polarity': 'negative'}]  \n",
       "4        [{'term': 'pizza', 'polarity': 'negative'}]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k4v8QMMT4B7t"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da980c3f78724d9788d7ec3df4439633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eedd726ebe24ab18802ff5829e16233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create T5 utils object\n",
    "t5_exp = T5Generator(model_checkpoint)\n",
    "\n",
    "# Tokenize Dataset\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = {\n",
    "    'output_dir':model_out_path,\n",
    "    'evaluation_strategy':\"epoch\",\n",
    "    'learning_rate':5e-5,\n",
    "    'lr_scheduler_type':'cosine',\n",
    "    'per_device_train_batch_size':8,\n",
    "    'per_device_eval_batch_size':16,\n",
    "    'num_train_epochs':4,\n",
    "    'weight_decay':0.01,\n",
    "    'warmup_ratio':0.1,\n",
    "    'save_strategy':'no',\n",
    "    'load_best_model_at_end':False,\n",
    "    'push_to_hub':False,\n",
    "    'eval_accumulation_steps':1,\n",
    "    'predict_with_generate':True,\n",
    "    'use_mps_device':use_mps,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 590\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 254\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It'll giving us an error if we don't have a validation set (when we try to train). Let's create one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the test dataset in half\n",
    "train_test_split = id_tokenized_ds['test'].train_test_split(test_size=0.5)\n",
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 590\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['raw_text', 'aspectTerms', 'labels', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the \"new\" 'train' and 'test' splits to the original DatasetDict with their new names\n",
    "id_tokenized_ds['test'] = train_test_split['train']\n",
    "id_tokenized_ds['validation'] = train_test_split['test']  # Use 'test' as the validation set\n",
    "id_tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (24.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rwynn\\pycharmprojects\\instructabsa\\venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install -U accelerate\n",
    "!python -m pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tZr7BAy6eJOA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer device: cpu\n",
      "\n",
      "Model training started ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='296' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [296/296 1:40:30, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.295548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "model_trainer = t5_exp.train(id_tokenized_ds, **training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rqL4maz_Dlz"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy6aOHv4_FUo"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set1()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_joint_task_format(loader.train_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.joint['bos_instruct1'], instruct_handler.joint['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_joint_task_format(loader.test_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.joint['bos_instruct1'], instruct_handler.joint['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbcnbLXtt9Am"
   },
   "outputs": [],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator(model_out_path)\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBCUT9jDt8-Q"
   },
   "outputs": [],
   "source": [
    "p, r, f1, _ = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, _ = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate the basic model on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MT data\n",
    "id_train_file_path = './Dataset/SemEval14/Train/mt_train_data.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/Test/mt_test_data.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set2()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_aspe_format(loader.train_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_aspe_format(loader.test_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e098859a14240ef894682609713ccd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fb1eb37b1849319e35af3941d5498a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [09:31<00:00, 15.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:14<00:00, 16.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator('kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined')\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision:  0.7053320860617399\n",
      "Train Recall:  0.7717502558853634\n",
      "Train F1:  0.7370478983382209\n",
      "Test Precision:  0.680161943319838\n",
      "Test Recall:  0.7962085308056872\n",
      "Test F1:  0.7336244541484715\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, _ = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, _ = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the results:\n",
    "Train Precision:  0.7053320860617399\n",
    "\n",
    "Train Recall:  0.7717502558853634\n",
    "\n",
    "Train F1:  0.7370478983382209\n",
    "\n",
    "Test Precision:  0.680161943319838\n",
    "\n",
    "Test Recall:  0.7962085308056872\n",
    "\n",
    "Test F1:  0.7336244541484715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's load the model from our trained checkpoint and see the difference in how it performs on the same train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fa3125886e435abb2390071fe05417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953d70a469c0450a8559956786bc391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [09:22<00:00, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [04:40<00:00, 17.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator(model_out_path)\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision:  0.888125613346418\n",
      "Train Recall:  0.9263050153531218\n",
      "Train F1:  0.906813627254509\n",
      "Test Precision:  0.7851063829787234\n",
      "Test Recall:  0.8661971830985915\n",
      "Test F1:  0.8236607142857143\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, _ = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, _ = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the results:\n",
    "Train Precision:  0.888125613346418\n",
    "\n",
    "Train Recall:  0.9263050153531218\n",
    "\n",
    "Train F1:  0.906813627254509\n",
    "\n",
    "Test Precision:  0.7851063829787234\n",
    "\n",
    "Test Recall:  0.8661971830985915\n",
    "\n",
    "Test F1:  0.8236607142857143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "070d57aa6b4a039a680ca3535d2f37da5ed020b02d8ccf58fedcd4e32b3636b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
